# -*- coding: utf-8 -*-
"""Copy of Open_Source_LLMs_kp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rSpPVZ_szodK86sADB2dVj0hl4CfrNsJ
"""




import faiss
from numpy import random

from transformers import BertTokenizer, BertModel, logging
from sklearn.metrics.pairwise import cosine_similarity

from transformers import BitsAndBytesConfig
import torch
import random
from tqdm import tqdm


from sklearn.model_selection import train_test_split
import numpy as np
import os
import json
import pickle

from numpy import random

# import gradio as gr
# from gradio_client import Client
import openai
from tenacity import retry, stop_after_attempt, wait_random_exponential
import json



tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')
model_bert = BertModel.from_pretrained('bert-base-uncased')

logging.set_verbosity_error()

system_message = """The following is a conversation between a Human and an AI Assistant.
The assistant is helpful, respectful and honest, and it always answers as helpfully as possible, while being safe.
The Assistant's answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.
Please ensure that the Assistant's responses are socially unbiased and positive in nature.
If a question by the human does not make any sense, or is not factually coherent, the Assistant should explain why instead of answering something not correct.
If the Assistant does not know the answer to a question, please don't share false information.
####

"""

# Replace with your API keys and endpoint URLs
#api_keys = ["<API_KEY_1>", "<API_KEY_2>", "<API_KEY_3>"]
#endpoint_urls = ["<ENDPOINT_URL_1>", "<ENDPOINT_URL_2>", "<ENDPOINT_URL_3>"]
#llm_names = ["LLM 1", "LLM 2", "LLM 3"]

api_keys = ["EMPTY", "EMPTY", "EMPTY"]#, "EMPTY"]
endpoint_urls = ["https://6621-203-110-242-13.ngrok-free.app"]#["https://d06d-130-75-87-254.ngrok-free.app"]#, "https://akdeniz27-llama-2-70b-chat-hf-with-easyllm.hf.space/"]
llm_names = []

for api_key, endpoint_url in zip(api_keys, endpoint_urls):
    if 'hf.space' in endpoint_url:
        model_name = endpoint_url.replace('https://', '').replace('.hf.space', '').replace('/', '')
    else:
        openai.api_key = api_key
        openai.api_base = f"{endpoint_url}/v1"
        model_names = openai.Model.list()
        model_name = model_names["data"][0]["id"]
    llm_names.append(model_name)

# Function to retrieve LLM outputs using the given API key and endpoint
@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))
def get_completion(prompt, api_key, endpoint_url, hard_code_exception=False):
    # if new_sheet_name=='poem_properties':
    #     if hard_code_exception==True:
    #         max_tokens=128
    #     else:
    #         max_tokens=150
    # else:

    max_tokens=200
    if 'hf.space' in endpoint_url:
        client = Client(endpoint_url)
        result = client.predict(
                        prompt, # str in 'Message' Textbox component
                        api_name="/chat"
        )
        return result.strip()
    openai.api_key = api_key
    openai.api_base = f"{endpoint_url}/v1"
    model_names = openai.Model.list()
    model_name = model_names["data"][0]["id"]

    res = openai.Completion.create(
        model=model_name,  # Replace with your model name
        prompt=system_message + prompt,
        # messages=[
        #     {"role": "system", "content": system_message},
        #     {"role": "user", "content": prompt},
        # ],
        temperature=0.9,
        top_k=10,
        top_p=1.0,
        n=10,
        max_tokens=200,
    )
    out_text = []
    for x in range(0, 10):
        out_text.append(res['choices'][x]['text'].strip())
    return out_text



def compare_llm_outputs(user_query, hard_code_exception=False):
    # results = [get_completion(user_query, api_keys[i], endpoint_urls[i], hard_code_exception=hard_code_exception) for i in range(len(endpoint_urls))]
    results = get_completion(user_query, api_keys[0], endpoint_urls[0], hard_code_exception=hard_code_exception)

    return results
    # return res['choices'][0]['message']['content'].strip()


def self_con(tmp_list):
    ans_list = []
    for tmp in tmp_list:
        # tmp_list.append(compare_llm_outputs(user_query))
        # tmp = compare_llm_outputs(user_query)
        # print(tmp)
        ans = ""
        if len(tmp.split("The option is "))>1:
            ans = tmp.split("The option is ")[1][0]
            print(ans)
            # ans = ans.split("\n")[0]
        # ans = ans.replace("$", "")
        # ans = ans.strip()
        ans_list.append(ans)

    # print(ans_list)

    d = {}
    for i in ans_list:
        if i in d:
            d[i] += 1
        else:
            d[i] = 1
    print(d)
    n = sorted(d.items(), key=lambda x:x[1], reverse=True)
    return n


def mmr(doc_embeddings, query_embedding, lambda_param, top_k):
    """
    Maximal Marginal Relevance (MMR) function using Faiss.

    Parameters:
    - doc_embeddings: 2D array, each row represents the embedding of a document.
    - query_embedding: 1D array, the embedding of the query.
    - lambda_param: float, controls the trade-off between relevance and diversity.
    - top_k: int, the number of documents to return.

    Returns:
    - selected_indices: List of indices representing the selected documents.
    """

    # Normalize embeddings
    doc_embeddings = doc_embeddings / np.linalg.norm(doc_embeddings, axis=1, keepdims=True)
    query_embedding = query_embedding / np.linalg.norm(query_embedding)

    # Number of documents
    num_docs = doc_embeddings.shape[0]
    

    # Create an index for Faiss
    index = faiss.IndexFlatIP(doc_embeddings.shape[1])

    index.add(doc_embeddings.astype(np.float32))

    # Query the index for similar documents
    _, similarity_indices = index.search(query_embedding.reshape(1, -1).astype(np.float32), top_k)

    # Compute relevance scores
    relevance_scores = np.dot(doc_embeddings, query_embedding)

    # Initialize selected set
    selected_set = set()

    # Add the most relevant document to the selected set
    max_relevance_index = np.argmax(relevance_scores)
    selected_set.add(max_relevance_index)

    # Compute MMR scores and select documents iteratively
    while len(selected_set) < top_k:
        remaining_indices = list(set(similarity_indices[0]) - selected_set)
        remaining_embeddings = doc_embeddings[remaining_indices]

        # Compute similarity with the query for remaining documents
        similarity_scores = np.dot(remaining_embeddings, query_embedding)

        # Compute MMR scores
        mmr_scores = lambda_param * relevance_scores[remaining_indices] - (1 - lambda_param) * similarity_scores

        # Select document with maximum MMR score
        max_mmr_index = remaining_indices[np.argmax(mmr_scores)]
        selected_set.add(max_mmr_index)

    # Convert selected set to a list of indices
    selected_indices = list(selected_set)

    return selected_indices




def prompt_for_manual_prediction(ex, shots):
    stop_signal = "\n\n"
    showcase_examples = [
            "Q: {}\nO: {} \nA: {}. The option is {}\n".format(
                 s["question"],s["options"],
                s["rationale"], s["correct"]) for s in shots
        ]




    # prompt = "\n".join(showcase_examples)
    # prompt=prompt+"\n\n{text}\n"


    input_example = "\nQ: {}\n O: {}\nA:".format(ex['question'], ex['options'])
    prompt = "\n".join(showcase_examples + [input_example])

    return prompt, stop_signal



def in_context_manual_prediction(ex, training_data):
    template, stop_signal = prompt_for_manual_prediction(ex, training_data)
    #print("template:",template)

    outputs=compare_llm_outputs(template)
    print("outputs:",outputs)
    ans=outputs[0].split("The option is ")
   

    if(len(ans)>1):
      
      return ans[1][0].strip()
    else:
      return ""
    








def test_few_shot_manual_prediction():
    print("Running prediction")
    with open('AQUA_RAT/dev.jsonl', 'r') as json_file1:
        json_list1 = list(json_file1)
    dev_set=[]


    for json_str in json_list1:
        result1 = json.loads(json_str)
        dev_set.append(result1)
    # dev_set =open("AQUA_RAT/test.json")
    # dev_set=json.load(dev_set)
    #dev_set=dev_set[:2]


    with open('AQUA_RAT/train.jsonl', 'r') as json_file:
        json_list = list(json_file)

    train_set=[]


    random.seed(7)


    for json_str in json_list:
        result = json.loads(json_str)
        train_set.append(result)
    #train_set=train_set[:20]


    doc_embeddings=[]
    

    for i in train_set:
        inputs_sentence1 = tokenizer_bert(i["question"], return_tensors='pt', padding=True, truncation=True)
        with torch.no_grad():
            outputs_sentence1 = model_bert(**inputs_sentence1)
        embedding_sentence1 = outputs_sentence1.last_hidden_state.mean(dim=1).numpy()[0]
        doc_embeddings.append(embedding_sentence1)



    
    # context_similarity=0

    




    print("started Running:")
    matches=0
    mismatches=0

    for ex in tqdm(dev_set,total=len(dev_set),desc="predicting"):
        # for train_ex in train_set:
        #     context_similarity+=get_similarity(train_ex[])
       


        query = tokenizer_bert(ex['question'], return_tensors='pt', padding=True, truncation=True)
        with torch.no_grad():
            outputs_sentence1 = model_bert(**query)
        query_embedding=outputs_sentence1.last_hidden_state.mean(dim=1).numpy()[0]

        # Set the lambda parameter (trade-off between relevance and diversity)
        lambda_param = 0.5

        # Set the number of documents to return
        top_k = 5

        # Apply MMR function
        selected_indices = mmr(doc_embeddings, query_embedding, lambda_param, top_k)
        print("selected indices:",selected_indices)
        train_set_1=[]
        for i in selected_indices:
          train_set_1.append(train_set[i])
        user_query,stop=prompt_for_manual_prediction(ex,train_set_1)

        tmp_list = compare_llm_outputs(user_query)
        # print(len(tmp_list))
        
        n = self_con(tmp_list)
        answer = n[0][0]
        if answer=="" and len(n)>1: answer = n[1][0]
        print("\nAnswer: ", answer)
        gt = ex["correct"]
        print("GT: ", gt)
        if(answer==gt):
          matches+=1
        else:
          mismatches+=1
    print("EM:",matches/(matches+mismatches))

test_few_shot_manual_prediction()







